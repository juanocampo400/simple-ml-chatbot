{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1 - Setup"
      ],
      "metadata": {
        "id": "w9vkxdaBeZk3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDTIem2ScxyQ",
        "outputId": "05e808e2-2906-44eb-fbea-8e96f08c6e08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# mount + imports\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_ROOT = '/content/drive/My Drive/ChatBot'\n",
        "\n",
        "import json, string, random\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# NLTK data needed by word_tokenize + lemmatizer\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# optional but useful for lemmatizer language data\n",
        "nltk.download(\"omw-1.4\")\n",
        "\n",
        "# load intents\n",
        "with open(f'{DATA_ROOT}/intents.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "# initializing lemmatizer to get stem of words\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 - Data Prep (vocab + training set)"
      ],
      "metadata": {
        "id": "QgSbXZcTecy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build words/classes from intents\n",
        "words = [] #for bag-of-words (BoW) model/ vocab for patterns\n",
        "classes = [] #for BoW model/ vocab for tags\n",
        "data_X = [] #for storing patterns\n",
        "data_Y = [] #for storing tag corresponding to pattern in data_X\n",
        "\n",
        "# iterating for all intents\n",
        "for intent in data[\"intents\"]:\n",
        "  for pattern in intent[\"patterns\"]:\n",
        "    tokens = nltk.word_tokenize(pattern) # tokenizing patterns\n",
        "    words.extend(tokens) # appending tokens to words\n",
        "    data_X.append(pattern) # appending patterns to data_X\n",
        "    data_Y.append(intent[\"tag\"]) # appending associated tag to patterns\n",
        "\n",
        "  # adding tag to classes if not already there\n",
        "  if intent[\"tag\"] not in classes:\n",
        "    classes.append(intent[\"tag\"])\n",
        "\n",
        "# lemmatizing words in vocab and converting to lowercase\n",
        "# if words don't appear in punctuation\n",
        "words = [lemmatizer.lemmatize(word.lower())\n",
        "  for word in words if word not in string.punctuation]\n",
        "\n",
        "# sorting vocab and classes in alphabetical order, taking # set to ensure no duplicates\n",
        "words = sorted(set(words))\n",
        "classes = sorted(set(classes))\n",
        "\n",
        "# converting text to numbers, building training set via BoW\n",
        "training = []\n",
        "out_empty = [0] * len(classes)\n",
        "\n",
        "# creating BoW model\n",
        "\n",
        "for idx, doc in enumerate(data_X):\n",
        "  # 1) tokenize each training sentence (pattern)\n",
        "  #    e.g., \"How are you?\" -> [\"How\", \"are\", \"you\", \"?\"]\n",
        "  tokens = nltk.word_tokenize(doc)\n",
        "\n",
        "  # 2) normalize tokens (lowercase + lemmatize) and remove punctuation\n",
        "  #    ensures \"running\" -> \"run\", \"Hello\" -> \"hello\"\n",
        "  tokens = [lemmatizer.lemmatize(w.lower())\n",
        "    for w in tokens if w not in string.punctuation]\n",
        "\n",
        "  # 3) build Bo@ vector for this sentence\n",
        "  #    go through the *entire vocabulary* (words)\n",
        "  #    put 1 if vocab word appears in this sentence's tokens, else 0\n",
        "  bow = [1 if w in tokens else 0 for w in words]\n",
        "\n",
        "  # 4) build the one-hot label vector for sentence's intent (class)\n",
        "  label = list(out_empty)\n",
        "  label[classes.index(data_Y[idx])] = 1\n",
        "\n",
        "  # 5) add this training example (features + label) to the dataset\n",
        "  training.append([bow, label])\n",
        "\n",
        "# shuffle data and convert to array\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=object)\n",
        "\n",
        "# split features and target labels\n",
        "train_X = np.array(list(training[:, 0]))\n",
        "train_Y = np.array(list(training[:, 1]))\n"
      ],
      "metadata": {
        "id": "WW3WTYPbdo9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3A - Train & Save Model"
      ],
      "metadata": {
        "id": "q8NEGquIel6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Model definition\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_X[0]),), activation = \"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation = \"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_Y[0]), activation = \"softmax\"))\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=[\"accuracy\"])\n",
        "print(model.summary())\n",
        "\n",
        "# Train\n",
        "model.fit(x=train_X, y=train_Y, epochs=150, verbose=1)\n",
        "\n",
        "# persist model + vocab so we can skip retraining later\n",
        "MODEL_PATH = f'{DATA_ROOT}/model.keras' # .keras format preferred\n",
        "VOCAB_PATH = f'{DATA_ROOT}/vocab.json'\n",
        "\n",
        "model.save(MODEL_PATH)\n",
        "with open(VOCAB_PATH, 'w') as f:\n",
        "  json.dump({'words': words, 'classes': classes}, f)\n",
        "\n",
        "print(\"Saved:\", MODEL_PATH, \"and\", VOCAB_PATH)\n"
      ],
      "metadata": {
        "id": "ulpm5dLyd8FC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4235d40-faec-47b4-8b7b-36900da0166b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m10,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)             │         \u001b[38;5;34m1,170\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,170</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,922\u001b[0m (77.82 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,922</span> (77.82 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,922\u001b[0m (77.82 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,922</span> (77.82 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.0434 - loss: 2.9315\n",
            "Epoch 2/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.1425 - loss: 2.7916\n",
            "Epoch 3/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2198 - loss: 2.7087 \n",
            "Epoch 4/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2971 - loss: 2.5101\n",
            "Epoch 5/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2980 - loss: 2.4023\n",
            "Epoch 6/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3501 - loss: 2.2470 \n",
            "Epoch 7/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3284 - loss: 2.1366 \n",
            "Epoch 8/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5169 - loss: 1.8270 \n",
            "Epoch 9/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5169 - loss: 1.7051\n",
            "Epoch 10/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5839 - loss: 1.3968 \n",
            "Epoch 11/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.4405 - loss: 1.5424 \n",
            "Epoch 12/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6472 - loss: 1.2435 \n",
            "Epoch 13/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7689 - loss: 0.8896\n",
            "Epoch 14/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7350 - loss: 1.0165\n",
            "Epoch 15/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7802 - loss: 0.7955\n",
            "Epoch 16/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6594 - loss: 1.0184\n",
            "Epoch 17/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7263 - loss: 0.9091 \n",
            "Epoch 18/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7020 - loss: 0.9299 \n",
            "Epoch 19/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9001 - loss: 0.5948\n",
            "Epoch 20/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8236 - loss: 0.6186 \n",
            "Epoch 21/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9331 - loss: 0.3960\n",
            "Epoch 22/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8123 - loss: 0.4916 \n",
            "Epoch 23/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8227 - loss: 0.5029\n",
            "Epoch 24/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8688 - loss: 0.4240\n",
            "Epoch 25/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9001 - loss: 0.3549\n",
            "Epoch 26/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8349 - loss: 0.4950\n",
            "Epoch 27/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9340 - loss: 0.3193\n",
            "Epoch 28/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9557 - loss: 0.2984\n",
            "Epoch 29/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9001 - loss: 0.3291\n",
            "Epoch 30/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8584 - loss: 0.4158\n",
            "Epoch 31/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8801 - loss: 0.3013\n",
            "Epoch 32/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9783 - loss: 0.2098\n",
            "Epoch 33/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9783 - loss: 0.2168\n",
            "Epoch 34/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8792 - loss: 0.3511\n",
            "Epoch 35/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9131 - loss: 0.2658\n",
            "Epoch 36/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9018 - loss: 0.3261\n",
            "Epoch 37/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9557 - loss: 0.1549\n",
            "Epoch 38/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9557 - loss: 0.1598\n",
            "Epoch 39/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8801 - loss: 0.3153\n",
            "Epoch 40/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9566 - loss: 0.1826\n",
            "Epoch 41/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9783 - loss: 0.1214\n",
            "Epoch 42/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9018 - loss: 0.2397\n",
            "Epoch 43/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9566 - loss: 0.1307\n",
            "Epoch 44/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9340 - loss: 0.1665\n",
            "Epoch 45/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9566 - loss: 0.0889\n",
            "Epoch 46/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9557 - loss: 0.1520\n",
            "Epoch 47/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9444 - loss: 0.1653\n",
            "Epoch 48/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9340 - loss: 0.1553\n",
            "Epoch 49/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9774 - loss: 0.1035\n",
            "Epoch 50/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9453 - loss: 0.1575\n",
            "Epoch 51/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0613\n",
            "Epoch 52/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9670 - loss: 0.1484\n",
            "Epoch 53/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0481\n",
            "Epoch 54/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9557 - loss: 0.1520\n",
            "Epoch 55/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9670 - loss: 0.0826\n",
            "Epoch 56/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0344\n",
            "Epoch 57/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9661 - loss: 0.1351\n",
            "Epoch 58/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9670 - loss: 0.1010\n",
            "Epoch 59/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9783 - loss: 0.0795\n",
            "Epoch 60/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0298\n",
            "Epoch 61/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9453 - loss: 0.1165\n",
            "Epoch 62/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9670 - loss: 0.0852\n",
            "Epoch 63/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9236 - loss: 0.1841\n",
            "Epoch 64/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9010 - loss: 0.1962\n",
            "Epoch 65/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9453 - loss: 0.1807\n",
            "Epoch 66/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9887 - loss: 0.0692\n",
            "Epoch 67/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0544\n",
            "Epoch 68/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9349 - loss: 0.1636\n",
            "Epoch 69/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9887 - loss: 0.0586\n",
            "Epoch 70/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9783 - loss: 0.0919\n",
            "Epoch 71/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9887 - loss: 0.0442\n",
            "Epoch 72/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9349 - loss: 0.1677\n",
            "Epoch 73/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9557 - loss: 0.1211\n",
            "Epoch 74/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0471\n",
            "Epoch 75/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9887 - loss: 0.0343\n",
            "Epoch 76/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9783 - loss: 0.0691\n",
            "Epoch 77/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9340 - loss: 0.1446\n",
            "Epoch 78/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0734\n",
            "Epoch 79/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9453 - loss: 0.1254\n",
            "Epoch 80/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9783 - loss: 0.0635\n",
            "Epoch 81/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9566 - loss: 0.0676\n",
            "Epoch 82/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9236 - loss: 0.1552\n",
            "Epoch 83/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9783 - loss: 0.0526\n",
            "Epoch 84/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9887 - loss: 0.0419\n",
            "Epoch 85/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9783 - loss: 0.0435\n",
            "Epoch 86/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9783 - loss: 0.0450\n",
            "Epoch 87/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9566 - loss: 0.0780\n",
            "Epoch 88/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9887 - loss: 0.0354\n",
            "Epoch 89/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9670 - loss: 0.0655\n",
            "Epoch 90/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0391\n",
            "Epoch 91/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9670 - loss: 0.0655\n",
            "Epoch 92/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9783 - loss: 0.0297 \n",
            "Epoch 93/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9783 - loss: 0.0485\n",
            "Epoch 94/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9670 - loss: 0.1335\n",
            "Epoch 95/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9340 - loss: 0.1383\n",
            "Epoch 96/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9670 - loss: 0.0591\n",
            "Epoch 97/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0310\n",
            "Epoch 98/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9774 - loss: 0.0655\n",
            "Epoch 99/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0374\n",
            "Epoch 100/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9566 - loss: 0.0754\n",
            "Epoch 101/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9670 - loss: 0.1058\n",
            "Epoch 102/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9783 - loss: 0.1106\n",
            "Epoch 103/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9887 - loss: 0.0445\n",
            "Epoch 104/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9887 - loss: 0.0294\n",
            "Epoch 105/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9783 - loss: 0.0741\n",
            "Epoch 106/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9566 - loss: 0.0941\n",
            "Epoch 107/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0115\n",
            "Epoch 108/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0210\n",
            "Epoch 109/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0328\n",
            "Epoch 110/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9783 - loss: 0.0649\n",
            "Epoch 111/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9670 - loss: 0.0709\n",
            "Epoch 112/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9887 - loss: 0.0329\n",
            "Epoch 113/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9331 - loss: 0.1406\n",
            "Epoch 114/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0205\n",
            "Epoch 115/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0294\n",
            "Epoch 116/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9566 - loss: 0.0658\n",
            "Epoch 117/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0103\n",
            "Epoch 118/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9774 - loss: 0.0827\n",
            "Epoch 119/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9783 - loss: 0.0514\n",
            "Epoch 120/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0252\n",
            "Epoch 121/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9887 - loss: 0.0728\n",
            "Epoch 122/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9783 - loss: 0.0456\n",
            "Epoch 123/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0179\n",
            "Epoch 124/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0115\n",
            "Epoch 125/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0157\n",
            "Epoch 126/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9887 - loss: 0.0457\n",
            "Epoch 127/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0237\n",
            "Epoch 128/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9783 - loss: 0.0984\n",
            "Epoch 129/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9887 - loss: 0.0244\n",
            "Epoch 130/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0235\n",
            "Epoch 131/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9670 - loss: 0.1238\n",
            "Epoch 132/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0242\n",
            "Epoch 133/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0097\n",
            "Epoch 134/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9670 - loss: 0.0705\n",
            "Epoch 135/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0215\n",
            "Epoch 136/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9887 - loss: 0.0816\n",
            "Epoch 137/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0286\n",
            "Epoch 138/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9783 - loss: 0.0368\n",
            "Epoch 139/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9783 - loss: 0.0957\n",
            "Epoch 140/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0243\n",
            "Epoch 141/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9887 - loss: 0.0473\n",
            "Epoch 142/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9783 - loss: 0.0300\n",
            "Epoch 143/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0204\n",
            "Epoch 144/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0124\n",
            "Epoch 145/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9887 - loss: 0.0274\n",
            "Epoch 146/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0362\n",
            "Epoch 147/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0413\n",
            "Epoch 148/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9783 - loss: 0.0415\n",
            "Epoch 149/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9783 - loss: 0.0545\n",
            "Epoch 150/150\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9783 - loss: 0.0393\n",
            "Saved: /content/drive/My Drive/ChatBot/model.keras and /content/drive/My Drive/ChatBot/vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3B - Reload Model (use instead of training)"
      ],
      "metadata": {
        "id": "qzfkmEOzeqm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this INSTEAD of Cell 3A above in a fresh session\n",
        "from tensorflow.keras.models import load_model\n",
        "MODEL_PATH = f'{DATA_ROOT}/model.keras'\n",
        "model = load_model(MODEL_PATH)\n",
        "print(\"Model loaded.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BQ51OuEaeClw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a63bb8f-30b0-4ef1-976f-6dbb47dc8d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3B.1 - Reload Vocab"
      ],
      "metadata": {
        "id": "dAz2td19xFhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_PATH = f'{DATA_ROOT}/vocab.json'\n",
        "with open(VOCAB_PATH) as f:\n",
        "    d = json.load(f)\n",
        "words, classes = d['words'], d['classes']\n",
        "print(\"Vocab loaded. | words:\", len(words), \"| classes:\", len(classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apKxXjINwxsV",
        "outputId": "16ec92b8-5b6f-4aad-b1be-1733317799a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab loaded. | words: 81 | classes: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3B.1 - Sanity Check"
      ],
      "metadata": {
        "id": "3CCRqibYxLHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert model.output_shape[-1] == len(classes), \"Model/classes size mismatch.\"\n",
        "print(\"OK: model outputs =\", model.output_shape[-1], \"| classes =\", len(classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw1XXcLVxKgz",
        "outputId": "ddf88f82-aa81-495d-c3de-01a86dc99081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: model outputs = 18 | classes = 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4 - Helper Functions"
      ],
      "metadata": {
        "id": "eXjwZK4OewGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-processing user input\n",
        "def clean_text(text):\n",
        "  # tokenize -> lowercase -> lemmatize -> drop punctuation\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  return [lemmatizer.lemmatize(w.lower()) for w in tokens if w not in string.punctuation]\n",
        "\n",
        "\n",
        "def bag_of_words(text, vocab):\n",
        "  # vector length == len(vocab); 1 if token present else 0\n",
        "  tokens = clean_text(text)\n",
        "  bow = [0] * len(vocab)\n",
        "  return np.array([1 if w in tokens else 0 for w in vocab])\n",
        "\n",
        "def pred_class(text, vocab, labels):\n",
        "  \"\"\"\n",
        "  Map a raw user message to one or more intent labels, ranked by probability.\n",
        "\n",
        "  Inputs:\n",
        "    text   : raw user message (string)\n",
        "    vocab  : list of vocabulary terms used to build BoW vectors\n",
        "    labels : list of class names (intent tags), aligned with model outputs\n",
        "\n",
        "  Outputs:\n",
        "    A list of intent labels whose probability > threshold, sorted desc by prob.\n",
        "    Empty list if nothing clears the threshold (lets get_response handle fallback).\n",
        "  \"\"\"\n",
        "\n",
        "  # 1) convert raw text to same BoW representation used in training\n",
        "  bow = bag_of_words(text, vocab) #shape: (len(vocab),)\n",
        "\n",
        "  # 2) model inference -> probability distribution over classes\n",
        "  #    verbose = 0 keeps Colab output clean\n",
        "  probs = model.predict(np.array([bow]), verbose=0)[0] #shape: (num_classes,)\n",
        "\n",
        "  # 3) keep only confident predctions\n",
        "  thresh = 0.5              # tune as needed (0.3..0.7 typical)\n",
        "  #    collect (class_index, probability) for items above threshold\n",
        "  candidates = [[i,p] for i ,p in enumerate(probs) if p > thresh]\n",
        "\n",
        "  # 4) rank by probability, highest first\n",
        "  candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  # 5) return labels for ranked candidates\n",
        "  return [labels[i] for i, _ in candidates]\n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "  \"\"\"\n",
        "  Select a response for the top predicted intent.\n",
        "  Falls back to a default message if no intent passes threshold\n",
        "  or the tag is missing in the intents file.\n",
        "  \"\"\"\n",
        "\n",
        "  # 1) no intents predicted above threshold -> return fallback\n",
        "  if not intents_list:\n",
        "    return \"Sorry! I don't understand.\"\n",
        "\n",
        "  # 2) use the highest probability intent (index 0 from pred_class)\n",
        "  top_tag = intents_list[0]\n",
        "\n",
        "  # 3) find the matching intent block in the loaded intents JSON\n",
        "  for intent in intents_json.get(\"intents\", []):\n",
        "      if intent.get(\"tag\") == top_tag:\n",
        "        # 4) pick a random canned response for variability\n",
        "        responses = intent.get(\"responses\", [])\n",
        "        if responses:\n",
        "          return random.choice(responses)\n",
        "        # tag found but no responses defined -> safe fallback\n",
        "        return \"Sorry! I don't understand.\"\n",
        "  # 5) tag not found in JSON (data drift or mismatch) -> fallback\n",
        "  return \"Sorry! I don't understand.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "_WaPNa-BeIB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1 - Skills Helper"
      ],
      "metadata": {
        "id": "o7GURtnI8LPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import skill helpers (rule-based)\n",
        "import re, ast, operator as op\n",
        "\n",
        "# Safe math eval: allow + - * / // % ** and parentheses only\n",
        "_allowed_ops = {\n",
        "    ast.Add: op.add, ast.Sub: op.sub, ast.Mult: op.mul, ast.Div: op.truediv,\n",
        "    ast.FloorDiv: op.floordiv, ast.Mod: op.mod, ast.Pow: op.pow, ast.USub: op.neg\n",
        "}\n",
        "def _eval_ast(node):\n",
        "    if isinstance(node, ast.Num):  # 3.8: ast.Constant in newer Python; Colab Py3.12 still supports Num for ints\n",
        "        return node.n\n",
        "    if isinstance(node, ast.UnaryOp) and type(node.op) in _allowed_ops:\n",
        "        return _allowed_ops[type(node.op)](_eval_ast(node.operand))\n",
        "    if isinstance(node, ast.BinOp) and type(node.op) in _allowed_ops:\n",
        "        return _allowed_ops[type(node.op)](_eval_ast(node.left), _eval_ast(node.right))\n",
        "    raise ValueError(\"disallowed\")\n",
        "\n",
        "def try_math(message: str):\n",
        "    expr = message.strip().replace(\" \", \"\")\n",
        "    # accept forms like 1+2, 10-7, 3*4, 12/3, -5+2, (2+3)*4, 2**3\n",
        "    if re.fullmatch(r\"[0-9\\(\\)\\+\\-\\*/% ]+|\\-?[0-9]+(\\.\\d+)?([+\\-*/%]\\-?[0-9]+(\\.\\d+)?)*\", message.strip()):\n",
        "        try:\n",
        "            val = _eval_ast(ast.parse(expr, mode=\"eval\").body)\n",
        "            return f\"{val}\"\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "CAPITALS = {\n",
        "    \"norway\": \"Oslo\",\n",
        "    \"sweden\": \"Stockholm\",\n",
        "    \"denmark\": \"Copenhagen\",\n",
        "    \"finland\": \"Helsinki\",\n",
        "    \"france\": \"Paris\",\n",
        "    \"spain\": \"Madrid\",\n",
        "    \"germany\": \"Berlin\",\n",
        "    \"italy\": \"Rome\",\n",
        "    \"japan\": \"Tokyo\",\n",
        "    \"united states\": \"Washington, D.C.\",\n",
        "    \"usa\": \"Washington, D.C.\",\n",
        "    \"canada\": \"Ottawa\",\n",
        "    \"mexico\": \"Mexico City\",\n",
        "    \"india\": \"New Delhi\",\n",
        "    \"china\": \"Beijing\",\n",
        "    \"australia\": \"Canberra\",\n",
        "    \"uk\": \"London\",\n",
        "    \"united kingdom\": \"London\"\n",
        "}\n",
        "_capital_pat = re.compile(r\"(?:what\\s+is\\s+)?the\\s+capital\\s+of\\s+(.+)\\??\", re.I)\n",
        "\n",
        "def try_capital(message: str):\n",
        "    m = _capital_pat.search(message)\n",
        "    if not m:\n",
        "        return None\n",
        "    country = m.group(1).strip().lower()\n",
        "    # normalize some punctuation and quotes\n",
        "    country = re.sub(r\"[^\\w\\s\\.]\", \"\", country)\n",
        "    # simple lookup\n",
        "    cap = CAPITALS.get(country)\n",
        "    return cap if cap else \"I don’t have that country in my local list.\"\n"
      ],
      "metadata": {
        "id": "pnKvPW8E8IJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5 - Chat Loop"
      ],
      "metadata": {
        "id": "Z08JXrRNe1aF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# interacting with chatbot\n",
        "# keep this loop in its own cell in Colab.\n",
        "# it reads user input and prints chatbot replies until you type \"0\".\n",
        "\n",
        "print(\"Press 0 if you don't want to chat with the ChatBot.\")\n",
        "\n",
        "while True:\n",
        "  try:\n",
        "    # prompt user for input (single line)\n",
        "    message = input(\"You: \").strip()\n",
        "  except EOFError:\n",
        "    # happens if input is interrupted or cell stopped\n",
        "    print(\"Chat ended unexpectedly.\")\n",
        "    break\n",
        "\n",
        "  # compare to the string \"0\", not the integer 0\n",
        "  if message == \"0\":\n",
        "    print(\"Chat ended.\")\n",
        "    break\n",
        "\n",
        "  # try rule-based skills first\n",
        "  ans = try_math(message)\n",
        "  if ans is not None:\n",
        "      print(\"Bot:\", ans)\n",
        "      continue\n",
        "\n",
        "  ans = try_capital(message)\n",
        "  if ans is not None:\n",
        "      print(\"Bot:\", ans)\n",
        "      continue\n",
        "\n",
        "  # otherwise, fall back to intent model\n",
        "  # predict intent(s) from the user message using the trained model\n",
        "  intents = pred_class(message, words, classes)\n",
        "\n",
        "  # map top intent to a response (or fallback if none meet threshold)\n",
        "  result = get_response(intents, data)\n",
        "\n",
        "  # show bot reply\n",
        "  print(\"Bot:\", result)"
      ],
      "metadata": {
        "id": "bOcANvEOeNzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "037cf10a-5712-44d6-b7cc-21d953a3d59a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Press 0 if you don't want to chat with the ChatBot.\n",
            "You: hi\n",
            "Bot: Hey.\n",
            "You: hello\n",
            "Bot: Hi.\n",
            "You: wha'ts up\n",
            "Bot: Waiting for your next question.\n",
            "You: what do you do\n",
            "Bot: I can do many things. For example, ask me for the capital, currency, and area of a country; a random number; or to calculate a math problem.\n",
            "You: what's the capital of america\n",
            "Bot: I don’t have that country in my local list.\n",
            "You: what about the US?\n",
            "Bot: I'm a simple chatbot trained on this intents file.\n",
            "You: what's the capitcal of the US>\n",
            "Bot: I work to serve you as well as possible.\n",
            "You: what's the capital of the US?\n",
            "Bot: I don’t have that country in my local list.\n",
            "You: what's the capital of Norway?\n",
            "Bot: Oslo\n",
            "You: what's the currency of Norway?\n",
            "Bot: I work to serve you as well as possible.\n",
            "You: what is 1+2?\n",
            "Bot: My job is to assist you.\n",
            "You: 1+2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3798308043.py:10: DeprecationWarning: ast.Num is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
            "  if isinstance(node, ast.Num):  # 3.8: ast.Constant in newer Python; Colab Py3.12 still supports Num for ints\n",
            "/tmp/ipython-input-3798308043.py:11: DeprecationWarning: Attribute n is deprecated and will be removed in Python 3.14; use value instead\n",
            "  return node.n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: 3\n",
            "You: 3*8\n",
            "Bot: 24\n",
            "You: what else do you do?\n",
            "Bot: I can do many things. For example, ask me for the capital, currency, and area of a country; a random number; or to calculate a math problem.\n",
            "You: i'm feeling good\n",
            "Bot: Good to hear. Anything you need?\n",
            "You: i'm feeling bad\n",
            "Bot: Noted. Do you want to talk about it?\n",
            "You: yes \n",
            "Bot: Sorry! I don't understand.\n",
            "You: I want to talk about how i feel bad\n",
            "Bot: Understood. Do you want resources or to change the topic?\n",
            "You: I want resources\n",
            "Bot: Good to hear. Anything you need?\n",
            "You: 0\n",
            "Chat ended.\n"
          ]
        }
      ]
    }
  ]
}